{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, urllib3\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spotify API credentials\n",
    "client_id = 'dfb4f42769e24d8a9870f81c6b660989'\n",
    "client_secret = \"PRIVATE INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "\n",
    "retry = urllib3.Retry(\n",
    "    total=0,\n",
    "    connect=None,\n",
    "    read=0,\n",
    "    allowed_methods=frozenset(['GET', 'POST', 'PUT', 'DELETE']),\n",
    "    status=0,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(429, 500, 502, 503, 504),\n",
    "    respect_retry_after_header=False  # <---\n",
    ")\n",
    "\n",
    "adapter = requests.adapters.HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "sp = spotipy.Spotify(\n",
    "    auth_manager=SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playlist ID - Top 50 Polska\n",
    "playlist_id = '37i9dQZEVXbN6itCcaL3Tt'\n",
    "# '37i9dQZF1DXcBWIGoYBM5M' - Top Global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Określ nazwę katalogu głównego projektu\n",
    "project_directory_name = 'BigDataProject'\n",
    "\n",
    "# Pobierz ścieżkę do katalogu, w którym znajduje się bieżący skrypt (SpotipyScript.ipynb)\n",
    "current_directory = os.path.dirname(os.path.abspath('__file__')).replace('\\\\','/')\n",
    "\n",
    "# Przejście w górę po drzewie katalogów, aż dojdziesz do katalogu projektu\n",
    "while os.path.basename(current_directory) != project_directory_name:\n",
    "    current_directory = os.path.dirname(current_directory).replace('\\\\','/')\n",
    "\n",
    "project_directory = current_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(track_info, csv_file_path):\n",
    "    # Sprawdź czy ścieżka kończy się na .csv\n",
    "    if not csv_file_path.lower().endswith('.csv'):\n",
    "        # Ustaw typ pliku na csv\n",
    "        csv_file_path = csv_file_path + \".csv\"\n",
    "\n",
    "    # Pobierz katalog z pełnej ścieżki\n",
    "    directory = os.path.dirname(csv_file_path)\n",
    "\n",
    "    # Sprawdź czy katalog istnieje, a jeśli nie, utwórz go (rekurencyjnie)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Jeśli dane wejściowe to DataFrame, użyj wbudowanej funkcji to_csv()\n",
    "    if isinstance(track_info, pd.DataFrame):\n",
    "        track_info.to_csv(csv_file_path, mode='w' if not os.path.exists(csv_file_path) else 'a', index=False)\n",
    "    else:\n",
    "        # Jeśli dane wejściowe to słownik, obsłuż ręcznie zapis do pliku CSV\n",
    "        fieldnames = list(track_info.keys())\n",
    "\n",
    "        with open(csv_file_path, mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames, delimiter=';')\n",
    "\n",
    "            # Sprawdź, czy plik CSV jest pusty, a jeśli tak, napisz nagłówki\n",
    "            if file.tell() == 0:\n",
    "                writer.writeheader()\n",
    "\n",
    "            # Zapisz informacje do pliku CSV\n",
    "            if isinstance(track_info[fieldnames[0]], list):\n",
    "                # Jeśli wartości są listami, przekształć listy wierszy\n",
    "                writer.writerows({key: value[i] for key, value in track_info.items()} for i in range(len(track_info[fieldnames[0]])))\n",
    "            else:\n",
    "                # Jeśli wartości nie są listami, zapisz jedną linię\n",
    "                writer.writerow(track_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_tracks(csv_file_path,sep=';',parse_dates=['date_added']):\n",
    "    # Wczytaj istniejące informacje o utworach z pliku CSV\n",
    "    try:\n",
    "        existing_tracks = pd.read_csv(csv_file_path, sep=sep, parse_dates=parse_dates)\n",
    "        \n",
    "        return existing_tracks\n",
    "    except FileNotFoundError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_new_rank(csv_file_path, top_tracks):\n",
    "    # Sprawdź datę dodania pierwszego utworu z playlisty\n",
    "    first_track_date_added = datetime.strptime(top_tracks['items'][0]['added_at'], '%Y-%m-%dT%H:%M:%SZ') if top_tracks['items'] else None\n",
    "\n",
    "    # Jeśli csv_file_path to ścieżka do katalogu, nadaj nazwę nowemu pliku\n",
    "    if os.path.isdir(csv_file_path):\n",
    "        csv_file_path = os.path.join(csv_file_path, f\"{first_track_date_added.date()}.csv\").replace('\\\\', '/')\n",
    "    else:\n",
    "        # Jeśli katalog nie istnieje, utwórz go\n",
    "        os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)\n",
    "        csv_file_path = csv_file_path.replace('\\\\', '/')\n",
    "\n",
    "    # Sprawdź, czy plik CSV istnieje\n",
    "    if os.path.exists(csv_file_path):\n",
    "        existing_tracks = load_existing_tracks(csv_file_path)\n",
    "        \n",
    "        # Sprawdź, czy plik istnieje, jest pusty, lub różni się datą od pierwszego utworu\n",
    "        if existing_tracks is None or existing_tracks.empty or existing_tracks['date_added'].iloc[-1].date() != first_track_date_added.date():\n",
    "            return True\n",
    "    else:\n",
    "        # Plik nie istnieje, więc na pewno istnieje nowy ranking\n",
    "        return True\n",
    "\n",
    "    # Istnieje nowy ranking\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_track_info(playlist_id, csv_file_path='data/'): \n",
    "    try:\n",
    "        # Pobierz top 50 utworów z playlisty\n",
    "        top_tracks = sp.playlist_tracks(playlist_id, limit=50)\n",
    "\n",
    "        # Sprawdź datę dodania pierwszego utworu z playlisty\n",
    "        first_track_date_added = datetime.strptime(top_tracks['items'][0]['added_at'], '%Y-%m-%dT%H:%M:%SZ') if top_tracks['items'] else None\n",
    "\n",
    "        # Stwórz zmienną informującą o istnieniu nowych danych\n",
    "        new_rank_exists = check_for_new_rank(csv_file_path, top_tracks)\n",
    "        \n",
    "        if not new_rank_exists:\n",
    "            return print(\"Brak nowych utworów do dodania.\")     \n",
    "        else:\n",
    "            # Jeśli csv_file_path to ścieżka do katalogu, nadaj nazwę nowemu pliku\n",
    "            if os.path.isdir(csv_file_path):\n",
    "                csv_file_path = os.path.join(csv_file_path, f\"{first_track_date_added.date()}.csv\").replace('\\\\', '/')\n",
    "            else:\n",
    "                # Jeśli katalog nie istnieje, utwórz go\n",
    "                os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)\n",
    "                csv_file_path = csv_file_path.replace('\\\\', '/')\n",
    "\n",
    "            # Przygotuj słownik informacji o utworach do zapisu\n",
    "            track_info_dict = {\n",
    "                'track_id': [track['track']['id'] for track in top_tracks['items']],\n",
    "                'rank': list(range(1, len(top_tracks['items']) + 1)),\n",
    "                'track_name': [track['track']['name'] for track in top_tracks['items']],\n",
    "                'artist': [track['track']['artists'][0]['name'] for track in top_tracks['items']],\n",
    "                'date_added': [datetime.strptime(track['added_at'], '%Y-%m-%dT%H:%M:%SZ') for track in top_tracks['items']],\n",
    "                'popularity': [track['track']['popularity'] for track in top_tracks['items']]\n",
    "            }\n",
    "            # Zapisz informacje o utworach do pliku CSV\n",
    "            save_to_csv(track_info_dict, csv_file_path)\n",
    "            \n",
    "            # Wyświetl informacje o utworach\n",
    "            for i in range(len(track_info_dict['rank'])):\n",
    "                print(f\"{track_info_dict['rank'][i]}. {track_info_dict['track_name'][i]} by {track_info_dict['artist'][i]} (Popularity: {track_info_dict['popularity'][i]})\")\n",
    "\n",
    "    except spotipy.SpotifyException as e:\n",
    "        print(f\"Spotify API error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_tracks_by_date(source_file_path, result_file_path=None): # IN CASE OF EMERGENCY - na wypadek usunięcia plików z danymi odrębnymi dla danej daty\n",
    "    # Funkcja rozdziela zbiorczy plik csv z top utworami, na pliki z top utworami dla danego dnia\n",
    "\n",
    "    # Wczytaj plik CSV, ze sprawdzeniem typu pliku\n",
    "    if not source_file_path.lower().endswith('.csv'):\n",
    "        return \"Plik nie jest plikiem csv\"\n",
    "    else:\n",
    "        df = pd.read_csv(source_file_path, parse_dates=['date_added'], sep=';')\n",
    "\n",
    "    # Zdefiniuj ścieżkę plików wynikowych\n",
    "    if not result_file_path:\n",
    "        result_file_path = os.path.dirname(source_file_path) +\"/\"\n",
    "\n",
    "    # Grupuj DataFrame według kolumny 'date_added'\n",
    "    grouped_by_date = df.groupby(df['date_added'].dt.date)\n",
    "\n",
    "    # Dla każdej grupy utwórz osobny plik CSV\n",
    "    for date, group in grouped_by_date:\n",
    "        # Utwórz nazwę pliku na podstawie daty\n",
    "        file_name = result_file_path + f\"{date}.csv\"\n",
    "        \n",
    "        # Zapisz grupę do pliku CSV\n",
    "        group.to_csv(file_name, index=False, sep=';')\n",
    "\n",
    "    print(\"Pliki CSV zostały utworzone dla każdego unikalnego dnia.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dopasuj ID utworów na podstawie wiersza z ramki - początkowo ramki dzienne nie zawierały track_id\n",
    "def get_track_id(row):\n",
    "    track_name = row['track_name']\n",
    "    artist = row['artist']\n",
    "    track_id = row.get('track_id')  # Sprawdź, czy track_id już istnieje\n",
    "\n",
    "    # Jeśli track_id już istnieje, nie wykonuj wyszukiwania\n",
    "    if track_id:\n",
    "        return track_id\n",
    "\n",
    "    # Wyszukiwanie utworu\n",
    "    results = sp.search(q=f\"track:{track_name} artist:{artist}\", type='track', limit=1)\n",
    "\n",
    "    # Sprawdź, czy znaleziono utwór\n",
    "    if results['tracks']['items']:\n",
    "        found_track = results['tracks']['items'][0]\n",
    "        track_id = found_track['id']\n",
    "        return track_id\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_track_id_to_csv(csv_file_path):\n",
    "    # Wczytaj ramkę danych CSV\n",
    "    df = load_existing_tracks(csv_file_path)\n",
    "\n",
    "    # Sprawdź, czy wczytano prawidłową ramkę danych\n",
    "    if df is not None:\n",
    "        # Zastosuj funkcję do każdego wiersza w ramce danych\n",
    "        df['track_id'] = df.apply(get_track_id, axis=1)\n",
    "\n",
    "        # Przesuń kolumnę 'id' na pierwszą pozycję\n",
    "        cols = ['track_id'] + [col for col in df if col != 'track_id']\n",
    "        df = df[cols]\n",
    "\n",
    "        # Zapisz nową ramkę danych pod starą ścieżką\n",
    "        df.to_csv(csv_file_path, sep=';', index=False)\n",
    "        print('Zmodyfikowano ramkę')\n",
    "    else:\n",
    "        print(\"Wczytywanie pliku CSV nie powiodło się.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_to_date_files(start_date,end_date,directory_path):\n",
    "    # Iterate through files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        try:\n",
    "            # Check if the file is a CSV file and its name represents a date within the specified range\n",
    "            if filename.endswith('.csv') and start_date <= filename <= end_date:\n",
    "                add_track_id_to_csv(file_path)\n",
    "                print(\"Done\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_date(date_str):\n",
    "    try:\n",
    "        datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_parquet(csv_file_path, output_dir, file_name, sep=';', parse_dates=['date_added']): \n",
    "    ### robocze zapisywanie plików do .parquet,  aby wczytywać je z hdfs przy pracy z pySpark ###\n",
    "\n",
    "    # Wczytaj istniejące informacje o utworach z funkcji\n",
    "    existing_tracks = load_existing_tracks(csv_file_path, sep, parse_dates)\n",
    "\n",
    "    if existing_tracks is not None:\n",
    "        if file_name is None:\n",
    "            # Utwórz nazwę pliku wynikowego w formacie Parquet\n",
    "            output_file_name = 'songs_' + csv_file_path.split('/')[-1].split('.')[0] + '.parquet'\n",
    "            output_file_path = os.path.join(output_dir, output_file_name).replace('\\\\','/')\n",
    "            \n",
    "            # Sprawdź, czy plik Parquet już istnieje\n",
    "            if os.path.exists(output_file_path):\n",
    "                return print(\"Taki plik już istnieje\")\n",
    "        else:\n",
    "            # Utwórz nazwę pliku wynikowego w formacie Parquet\n",
    "            output_file_path = os.path.join(output_dir, file_name + '.parquet').replace('\\\\','/')\n",
    "\n",
    "            # Sprawdź, czy plik Parquet już istnieje\n",
    "            if os.path.exists(output_file_path):\n",
    "                os.remove(output_file_path)  # Usuń istniejący plik\n",
    "                print('Znaleziono istniejący plik - zostanie on podmieniony')\n",
    "\n",
    "        # Zapisz dane w formacie Parquet\n",
    "        table = pa.Table.from_pandas(existing_tracks)\n",
    "        pq.write_table(table, output_file_path)\n",
    "\n",
    "        print(f\"Plik Parquet został utworzony: {output_file_path}\")\n",
    "    else:\n",
    "        print(\"Nie można znaleźć pliku CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_csv_to_parquet(input_directory, output_directory, file_name, sep=';', parse_dates=['date_added']): # wersja dla katalogu, który zawiera m.in. ramki daily\n",
    "    ### robocze zapisywanie plików do .parquet,  aby wczytywać je z hdfs przy pracy z pySpark ###\n",
    "\n",
    "    # Sprawdź, czy katalog istnieje\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Lista plików w katalogu wejściowym\n",
    "    files = os.listdir(input_directory)\n",
    "\n",
    "    for file in files:\n",
    "        # Sprawdź, czy plik ma odpowiedni format nazwy\n",
    "        if file.endswith('.csv') and is_valid_date(file.split('.')[0]):\n",
    "            # Utwórz pełną ścieżkę do pliku CSV\n",
    "            csv_path = os.path.join(input_directory, file)\n",
    "            convert_csv_to_parquet(csv_path, output_directory,file_name,sep= sep, parse_dates= parse_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_features(track_ids): # poprawianie ramki statycznej \n",
    "    \n",
    "    # Sprawdzenie czy track_ids jest listą lub stringiem\n",
    "    if not (type(track_ids) is list) and not (type(track_ids) is str):\n",
    "        return print(\"Podaj listę lub string z track_id\")\n",
    "\n",
    "    audio_features_list = sp.audio_features(tracks=track_ids)\n",
    "\n",
    "    # Przygotowanie danych do zapisu\n",
    "    audio_feat= {\n",
    "        'track_id': [audio_features['id'] for audio_features in audio_features_list],\n",
    "        'danceability': [audio_features['danceability'] for audio_features in audio_features_list],\n",
    "        'energy': [audio_features['energy'] for audio_features in audio_features_list],\n",
    "        'key': [audio_features['key'] for audio_features in audio_features_list],\n",
    "        'loudness': [audio_features['loudness'] for audio_features in audio_features_list],\n",
    "        'mode': [audio_features['mode'] for audio_features in audio_features_list],\n",
    "        'speechiness': [audio_features['speechiness'] for audio_features in audio_features_list],\n",
    "        'acousticness': [audio_features['acousticness'] for audio_features in audio_features_list],\n",
    "        'instrumentalness': [audio_features['instrumentalness'] for audio_features in audio_features_list],\n",
    "        'liveness': [audio_features['liveness'] for audio_features in audio_features_list],\n",
    "        'valence': [audio_features['valence'] for audio_features in audio_features_list],\n",
    "        'tempo': [audio_features['tempo'] for audio_features in audio_features_list],\n",
    "        'duration_ms': [audio_features['duration_ms'] for audio_features in audio_features_list]\n",
    "    }\n",
    "    return audio_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_info(track_ids): # poprawianie ramki statycznej \n",
    "    \"\"\"\n",
    "    Get track information for a list of track IDs.\n",
    "\n",
    "    Parameters:\n",
    "    - track_ids (list or str): List of track IDs or a single track ID as a string.\n",
    "\n",
    "    Returns:\n",
    "    - track_info (dict): Dictionary containing track information.\n",
    "    \"\"\"\n",
    "    # Sprawdzenie czy track_ids jest listą lub stringiem\n",
    "    if not (isinstance(track_ids, list) or isinstance(track_ids, str)):\n",
    "        raise ValueError(\"Podaj listę lub string z track_id\")\n",
    "\n",
    "    # Jeśli track_ids to string, zamień go na listę zawierającą ten string\n",
    "    if isinstance(track_ids, str):\n",
    "        track_ids = [track_ids]\n",
    "\n",
    "    # Pobierz informacje o utworach\n",
    "    tracks_info = sp.tracks(tracks=track_ids)['tracks']\n",
    "\n",
    "    # Przygotuj dane do zapisu\n",
    "    track_info = {\n",
    "        'track_id': [track['id'] for track in tracks_info],\n",
    "        'track_name': [track['name'] for track in tracks_info],\n",
    "        'track_artist': [track['artists'][0]['name'] for track in tracks_info],\n",
    "        'track_album_release_date': [track['album']['release_date'] for track in tracks_info],\n",
    "        'track_album_id': [track['album']['id'] for track in tracks_info],\n",
    "        'artist_id': [track['artists'][0]['id'] for track in tracks_info],\n",
    "        'track_album_name': [track['album']['name'] for track in tracks_info]\n",
    "    }\n",
    "\n",
    "    return track_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artist_genres(artist_ids): # poprawianie ramki statycznej \n",
    "    \"\"\"\n",
    "    Get genres for a list of artist IDs.\n",
    "\n",
    "    Parameters:\n",
    "    - artist_ids (list or str): List of artist IDs or a single artist ID as a string.\n",
    "\n",
    "    Returns:\n",
    "    - artist_genres (dict): Dictionary containing artist IDs and their associated genres.\n",
    "    \"\"\"\n",
    "    # Sprawdzenie czy artist_ids jest listą lub stringiem\n",
    "    if not (isinstance(artist_ids, list) or isinstance(artist_ids, str)):\n",
    "        raise ValueError(\"Podaj listę lub string z artist_id\")\n",
    "\n",
    "    # Jeśli artist_ids to string, zamień go na listę zawierającą ten string\n",
    "    if isinstance(artist_ids, str):\n",
    "        artist_ids = [artist_ids]\n",
    "\n",
    "    # Pobierz informacje o artystach\n",
    "    artists_info = sp.artists(artist_ids)['artists']\n",
    "\n",
    "    # Przygotuj dane do zapisu\n",
    "    artist_genres = {\n",
    "        'artist_id': [artist['id'] for artist in artists_info],\n",
    "        'artist_genres': [artist['genres'] if artist['genres'] else ['unknown'] for artist in artists_info]\n",
    "    }\n",
    "\n",
    "    return artist_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_data(track_ids): # poprawianie ramki statycznej \n",
    "    # Pobierz informacje o utworach\n",
    "    track_info = get_track_info(track_ids)\n",
    "    track_info_df = pd.DataFrame(track_info)\n",
    "\n",
    "    # Pobierz cechy audio utworów\n",
    "    audio_features = get_audio_features(track_ids)\n",
    "    audio_features_df = pd.DataFrame(audio_features)\n",
    "\n",
    "    # Połącz ramki danych na podstawie 'track_id'\n",
    "    full_data = pd.merge(track_info_df, audio_features_df, on='track_id', how='inner')\n",
    "\n",
    "    # Pobierz unikalne artist_ids\n",
    "    unique_artist_ids = full_data['artist_id'].unique()\n",
    "\n",
    "    # Pobierz informacje o gatunkach artystów\n",
    "    artist_genres = get_artist_genres(list(unique_artist_ids))\n",
    "    artist_genres_df = pd.DataFrame(artist_genres)\n",
    "\n",
    "\n",
    "    # Połącz ramki danych na podstawie 'artist_id'\n",
    "    full_data = pd.merge(full_data, artist_genres_df, on='artist_id', how='left')\n",
    "    print(f\" Dodano: {full_data.shape[0]} nowych utworów\")\n",
    "\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_song_base(csv_file_path, track_ids): # poprawianie ramki statycznej - dodawanie utworów z Top 50, których cech nie było oryginalnie w ramce\n",
    "    # Wczytaj istniejące dane z pliku CSV\n",
    "    if os.path.exists(csv_file_path):\n",
    "        existing_data = pd.read_csv(csv_file_path)\n",
    "        existing_track_ids = set(existing_data['track_id'].unique())\n",
    "    else:\n",
    "        # Jeśli plik nie istnieje, utwórz pustą ramkę danych\n",
    "        existing_data = pd.DataFrame()\n",
    "        existing_track_ids = set()\n",
    "\n",
    "    # Sprawdź, czy track_ids to lista, jeśli nie, zamień go na listę\n",
    "    if isinstance(track_ids, str):\n",
    "        track_ids = [track_ids]\n",
    "\n",
    "    # Wybierz unikalne track_id (bez duplikatów)\n",
    "    new_track_ids = set(track_ids) - existing_track_ids\n",
    "\n",
    "    # Jeśli są nowe track_id, pobierz ich pełne dane\n",
    "    if new_track_ids:\n",
    "        new_data = get_full_data(list(new_track_ids))\n",
    "\n",
    "        # Dołącz nowe dane do istniejących danych\n",
    "        updated_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "\n",
    "        # Zapisz zaktualizowane dane do pliku CSV\n",
    "        updated_data.to_csv(csv_file_path, index=False)\n",
    "        print(\"Dane zostały zaktualizowane.\")\n",
    "    else:\n",
    "        print(\"Brak nowych danych do dodania.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(list, chunk_size): # aby uniknąć przeciążenia API, gdy trzeba pobrać informacje o wielu utworach\n",
    "    return [list[i:i + chunk_size] for i in range(0, len(list), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_for_artist_info(chunk, csv_file_path): # poprawianie ramki statycznej \n",
    "    \"\"\"\n",
    "    Update the dataframe with track information for a given chunk of track IDs.\n",
    "\n",
    "    Parameters:\n",
    "    - chunk (list): List of track IDs.\n",
    "    - csv_file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - updated_dataframe (pd.DataFrame): Updated DataFrame with added track information.\n",
    "    \"\"\"\n",
    "    # Sprawdź czy plik istnieje\n",
    "    if os.path.exists(csv_file_path):\n",
    "        # Wczytaj istniejący plik CSV do ramki danych\n",
    "        dataframe = pd.read_csv(csv_file_path)\n",
    "    else:\n",
    "        # Jeśli plik nie istnieje, stwórz pustą ramkę danych\n",
    "        dataframe = pd.DataFrame()\n",
    "\n",
    "    # Sprawdź, czy track_id z aktualnego chunka już istnieje w ramce danych\n",
    "    existing_track_ids = set(dataframe['track_id'].unique())\n",
    "    new_track_ids = set(chunk) - existing_track_ids\n",
    "\n",
    "    if not new_track_ids:\n",
    "        print(\"Wszystkie track_id z tego chunka już istnieją w ramce danych. Pomijam.\")\n",
    "        return dataframe\n",
    "\n",
    "    # Pobierz informacje o utworach\n",
    "    track_info = get_track_info(list(new_track_ids))\n",
    "\n",
    "    # Pobierz artist_id z track_info\n",
    "    artist_ids = track_info['artist_id']\n",
    "\n",
    "    # Pobierz informacje o artystach\n",
    "    artist_genres = get_artist_genres(artist_ids)\n",
    "\n",
    "    # Stwórz słownik z informacjami o trackach, artystach i gatunkach\n",
    "    update_dict = {\n",
    "        'track_id': track_info['track_id'],\n",
    "        'artist_id': track_info['artist_id'],\n",
    "        'artist_genres': artist_genres['artist_genres']\n",
    "    }\n",
    "\n",
    "    # Jeśli dataframe jest pusty lub kolumny w update_dict zgadzają się z istniejącymi kolumnami,\n",
    "    # dołącz do niego nowe informacje\n",
    "    if dataframe.empty or set(update_dict.keys()).issubset(set(dataframe.columns)):\n",
    "        updated_dataframe = pd.concat([dataframe, pd.DataFrame(update_dict)], ignore_index=True)\n",
    "    else:\n",
    "        # Dodaj odpowiednie obsługi błędów lub dostosuj kolumny, jeśli to konieczne\n",
    "        print(\"Błąd: Kolumny w update_dict nie zgadzają się z istniejącymi kolumnami w dataframe.\")\n",
    "        updated_dataframe = dataframe\n",
    "\n",
    "    # Zapisz zaktualizowaną ramkę do pliku CSV\n",
    "    updated_dataframe.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    return updated_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunks(to_chunk_param,chunk_size=50, processing_function=None,  **kwargs):\n",
    "    track_id_chunks = chunks(kwargs.get(to_chunk_param, []), chunk_size)\n",
    "    total_chunks = len(track_id_chunks)\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for i, chunk in enumerate(track_id_chunks, start=1):\n",
    "        print(f\"Przetwarzanie chunka {i} z {total_chunks}\")\n",
    "        if processing_function is not None:\n",
    "            kwargs_copy = kwargs.copy()\n",
    "            kwargs_copy[to_chunk_param] = chunk\n",
    "            results_chunk = processing_function(**kwargs_copy)\n",
    "            results_list.append(results_chunk)\n",
    "        else:\n",
    "            print(\"Brak funkcji przetwarzającej.\")\n",
    "\n",
    "    return results_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_popularity(track_ids): # poprawianie ramki statycznej \n",
    "    \"\"\"\n",
    "    Get track information for a list of track IDs.\n",
    "\n",
    "    Parameters:\n",
    "    - track_ids (list, str, or DataFrame): List of track IDs, a single track ID as a string, or a DataFrame with a 'track_id' column.\n",
    "\n",
    "    Returns:\n",
    "    - track_info_df (DataFrame): DataFrame containing track information, including 'track_id' and 'popularity'.\n",
    "    \"\"\"\n",
    "    if isinstance(track_ids, pd.DataFrame):\n",
    "        # Handle DataFrame case\n",
    "        print(\"Processing DataFrame...\")\n",
    "        # Utwórz nową kolumnę w DataFrame: 'popularity'\n",
    "        track_ids = track_ids.copy()\n",
    "        track_ids['popularity'] = np.nan\n",
    "\n",
    "        # Znajdź wiersze, które zawierają NaN w kolumnie 'track_id'\n",
    "        nan_rows = track_ids['track_id'].isna()\n",
    "        num_nan_rows = nan_rows.sum()\n",
    "\n",
    "        if num_nan_rows > 0:\n",
    "            print(f\"{num_nan_rows} utworów nie posiada track id.\")\n",
    "\n",
    "        # Dla wierszy z NaN w kolumnie 'track_id', przypisz NaN do nowej kolumny\n",
    "        track_ids.loc[nan_rows, 'popularity'] = np.nan\n",
    "\n",
    "        # Dla wierszy bez NaN w kolumnie 'track_id', pobierz informacje o popularności z Spotify\n",
    "        non_nan_rows = ~nan_rows\n",
    "        if non_nan_rows.any():\n",
    "            non_nan_track_ids = track_ids.loc[non_nan_rows, 'track_id'].tolist()\n",
    "            tracks_info = sp.tracks(tracks=non_nan_track_ids)['tracks']\n",
    "            track_ids.loc[non_nan_rows, 'popularity'] = [track['popularity'] for track in tracks_info]\n",
    "\n",
    "    else:\n",
    "        # Handle list or string case\n",
    "        # Sprawdzenie czy track_ids jest listą lub stringiem\n",
    "        if not (isinstance(track_ids, list) or isinstance(track_ids, str)):\n",
    "            raise ValueError(\"Podaj listę, string lub DataFrame z track_id\")\n",
    "\n",
    "        # Jeśli track_ids to string, zamień go na listę zawierającą ten string\n",
    "        if isinstance(track_ids, str):\n",
    "            track_ids = [track_ids]\n",
    "\n",
    "        # Pobierz informacje o utworach\n",
    "        tracks_info = sp.tracks(tracks=track_ids)['tracks']\n",
    "\n",
    "        # Przygotuj dane do zapisu\n",
    "        track_info = {\n",
    "            'track_id': [track['id'] for track in tracks_info],\n",
    "            'popularity': [track['popularity'] for track in tracks_info]\n",
    "        }\n",
    "\n",
    "        # Zamień słownik na ramkę danych\n",
    "        track_ids = pd.DataFrame(track_info)\n",
    "\n",
    "    return track_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_yesterday_rank(excel_file_path): # na wypadek niepobrania danych z danego dnia można je wciąż uzyskać dnia następnego\n",
    "    ##ZAPISAIE DANYCH Z DNIA POPRZEDNIEGO NA PODSTAWIE https://kworb.net/spotify/country/pl_daily.html\n",
    "\n",
    "    # Zapisanie xlsx jako csv\n",
    "    print('-------------------------------------- Zapisz w formacie csv...')\n",
    "    # Wczytaj plik Excel\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "\n",
    "    # Zapisz do pliku CSV\n",
    "    csv_file_path = excel_file_path.replace('.xlsx','.csv')\n",
    "    df.to_csv(csv_file_path, index=False, sep=';')\n",
    "    # display(df)\n",
    "\n",
    "    # Rozdzielenie track_name i artist\n",
    "    print('\\n-------------------------------------- Rozdzielenie track_name i artist...')\n",
    "    # Funkcja do rozdzielenia kolumny 'Artist and Title'\n",
    "    def split_artist_title(row):\n",
    "        parts = row['Artist and Title'].split('-', 1)\n",
    "        artist = parts[0].strip()\n",
    "        track_name = parts[1].split('(w/', 1)[0].strip() if len(parts) > 1 else None\n",
    "        return pd.Series([artist, track_name], index=['artist', 'track_name'])\n",
    "\n",
    "    # Stosowanie funkcji do ramki danych\n",
    "    df[['artist', 'track_name']] = df.apply(split_artist_title, axis=1)\n",
    "\n",
    "    df.drop(columns='Artist and Title',axis=1, inplace=True)\n",
    "    # Zapisz do pliku CSV\n",
    "    # df.to_csv(csv_file_path, index=False, sep=';')\n",
    "    # display(df)\n",
    "\n",
    "    # Dodanie track_id na podstawie spotify_songs\n",
    "    print('\\n-------------------------------------- Dodanie track_id na podstawie spotify_songs...')\n",
    "    df2 = load_existing_tracks(project_directory +'/src/data/spotify_songs.csv', sep=',',parse_dates= False)\n",
    "    # Usuń duplikaty z df2\n",
    "    df2 = df2.drop_duplicates(subset=['track_artist', 'track_name'])\n",
    "\n",
    "    # Łączenie ramek danych na podstawie 'artist' i 'track_name'\n",
    "    df = pd.merge(df, df2[['track_artist','track_name', 'track_id']], how='left', left_on=['artist', 'track_name'], right_on=['track_artist', 'track_name'])\n",
    "\n",
    "    # Usuń dodatkowe kolumny z ramki df2\n",
    "    df.drop(['track_artist'], axis=1, inplace=True)\n",
    "\n",
    "    # Wyświetl wynikową ramkę danych\n",
    "    #Zapisz do pliku CSV\n",
    "    # df.to_csv(csv_file_path, index=False, sep=';')\n",
    "    # display(df)\n",
    "\n",
    "    #  Dodanie popularity\n",
    "    print('\\n-------------------------------------- Dodanie popularity...')\n",
    "    # Pobierz track_id z ramki danych\n",
    "    track_ids = df\n",
    "    track_info_list = process_chunks( # Wywołanie w pakietach aby uniknąć przeciążenia API\n",
    "        to_chunk_param='track_ids',\n",
    "        chunk_size=10,\n",
    "        processing_function=get_track_popularity,\n",
    "        track_ids = track_ids\n",
    "    )\n",
    "\n",
    "    # Połącz DataFrame'y z listy\n",
    "    df = pd.concat(track_info_list, ignore_index=True)\n",
    "\n",
    "    # Dodanie daty\n",
    "    print('\\n-------------------------------------- Dodanie daty...')\n",
    "\n",
    "    # Ścieżka do pliku top_tracks\n",
    "    df3_file_path = project_directory + '/src/data/top_tracks.csv'\n",
    "    # Wczytaj ramki danych\n",
    "    df3 = load_existing_tracks(df3_file_path)\n",
    "\n",
    "    # Pobierz nazwę pliku bez ścieżki i rozszerzenia\n",
    "    file_name = csv_file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    # Oblicz datę na podstawie nazwy pliku i daty z df3\n",
    "    day = (datetime.strptime(file_name, '%Y-%m-%d') - timedelta(days=7)).date()\n",
    "\n",
    "    # Zamień kolumnę 'date_added' na typ datetime\n",
    "    date_added = df3[df3['date_added'].dt.strftime('%Y-%m-%d') == str(day)]['date_added'].iloc[0] + timedelta(days=7)\n",
    "\n",
    "    # Dodaj kolumnę \"date_added\" do ramki df\n",
    "    df['date_added'] = date_added\n",
    "\n",
    "    print('\\n-------------------------------------- Poprawienie kolejności kolumn...')\n",
    "    # Załóżmy, że masz aktualną kolejność kolumn\n",
    "    current_columns = df.columns.tolist()\n",
    "\n",
    "    # Nowa kolejność kolumn\n",
    "    new_columns_order = ['track_id', 'rank', 'track_name', 'artist', 'date_added', 'popularity']\n",
    "\n",
    "    # Upewnij się, że wszystkie kolumny są obecne w nowej kolejności\n",
    "    assert set(current_columns) == set(new_columns_order), \"Nowa kolejność kolumn musi zawierać wszystkie istniejące kolumny.\"\n",
    "\n",
    "    # Ustaw nową kolejność kolumn w ramce danych\n",
    "    df = df[new_columns_order]\n",
    "\n",
    "    # Zapisz do pliku CSV\n",
    "    df.to_csv(csv_file_path, index=False, sep=';')\n",
    "    \n",
    "    return print(f\"Zapisano plik: {csv_file_path}. Zwróć uwagę na brakujące track_id i popularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------- Pobierz nowe rankingi --------------------------------------')\n",
    "# Pobieranie utworów do oddzielnej ramki dla danego dnia\n",
    "retrive_track_info(playlist_id,project_directory + '/src/data/csv/')\n",
    "\n",
    "# Pobieranie utworów do zbiorczej ramki\n",
    "retrive_track_info(playlist_id, project_directory + '/src/data/csv/top_tracks.csv')\n",
    "\n",
    "\n",
    "# Uzyskaj dzisiejszą datę\n",
    "todays_date = str(datetime.now().date())\n",
    "\n",
    "# Załaduj dzisiejszy ranking i ścieżkę do bazy\n",
    "todays_tracks_path = project_directory + '/src/data/csv/' + todays_date +'.csv'\n",
    "todays_tracks = load_existing_tracks(todays_tracks_path)\n",
    "base_path = project_directory + '/src/data/csv/spotify_songs.csv'\n",
    "\n",
    "print('\\n----------- Uzupełnij bazę piosenek --------------------------------------')\n",
    "# Uzupełnij bazę piosenek o nowe z danego dnia i ich atrybuty - baza bez duplikatów\n",
    "\n",
    "process_chunks( # Wywołanie w pakietach aby uniknąć przeciążenia API\n",
    "    to_chunk_param = 'track_ids', \n",
    "    chunk_size=50, \n",
    "    processing_function=update_song_base,\n",
    "    track_ids = list(set(todays_tracks['track_id'])),\n",
    "    csv_file_path = base_path)\n",
    "\n",
    "print('\\n----------- Zapisz w formacie parquet --------------------------------------')\n",
    "\n",
    "# Zamień format danych (bazy piosenek i pliku z dzisiejszym rankingiem) na .parquet i zapisz w odpowiednim katalogu\n",
    "convert_csv_to_parquet(todays_tracks_path,project_directory+'/src/data/parquet/',file_name=None) # dzisiejszy ranking\n",
    "convert_csv_to_parquet(base_path,os.path.dirname(base_path),file_name='spotify_songs',sep=',',parse_dates=False) # baza\n",
    "\n",
    "# # Zamień wszystkie pliki danych z datą w nazwie na .parquet i zapisz w specjalnym katalogu\n",
    "# convert_data_csv_to_parquet(project_directory+'/src/data/',project_directory+'/src/data/parquet/',file_name=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
